name: CI

on: [push, pull_request]

env:
  PIP_CACHE: ~/.cache/pip
  INDEX_DIR: data/index
  ENABLE_DENSE: "0"
  AE_BIND_PORT: "8001"
  CONCEPT_MIN_SCORE: "0.05"

jobs:
  test-api:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      PYTHONUNBUFFERED: "1"
      PYTHONHASHSEED: "0"
      ENVIRONMENT: "development"
      DEBUG: "true"
      ENABLE_DENSE: "0"
      AE_BIND_PORT: "8001"
      AE_INDEX_DIR: "${{ github.workspace }}/data/index"
      AE_CACHE_ENABLED: "0"
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install (editable)
        run: |
          python -m pip install -U pip wheel
          pip install -e .[dev] || pip install -e .

      - name: Prepare index
        run: |
          set -euo pipefail
          ./scripts/sync_rfc_min.sh
          python scripts/build_index.py
          ls -l data/index || true

      - name: CI diagnostics
        run: |
          echo "Python:"; python -V
          echo "PWD:"; pwd
          echo "Tree (top-level):"; ls -la
          echo "ae2/router files:"; ls -la ae2/router || true
          echo "Show definitional router:"; sed -n '1,120p' ae2/router/definitional_router.py || true
          echo "contracts/models.py Query class:"; awk '/class Query\(BaseModel\)/,/^$/' ae2/contracts/models.py || true
          echo "Installed package location:"; python -c "import ae2,inspect,os; import ae2.router.definitional_router as dr; print(ae2.__file__); print(inspect.getsource(dr.DefinitionalRouter))"

      - name: Run tests (pytest)
        run: |
          set -euo pipefail
          mkdir -p .pytest
          # full output + first failure, deterministic order
          pytest -q --maxfail=1 --disable-warnings -rA | tee .pytest/pytest.out

      - name: Upload pytest log
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pytest-log
          path: .pytest/pytest.out

  eval:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: test-api
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ${{ env.PIP_CACHE }}
          key: pip-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}

      - name: Install (editable)
        run: pip install -e .

      - name: Cache RFC index
        uses: actions/cache@v4
        with:
          path: ${{ env.INDEX_DIR }}
          key: rfcindex-${{ hashFiles('data/index/manifest.json') }}
          restore-keys: |
            rfcindex-

      - name: Build index (if needed)
        run: |
          if [ ! -f "${INDEX_DIR}/sections.jsonl" ]; then
            ./scripts/sync_rfc_min.sh
            python scripts/build_index.py
          fi

      - name: Run evaluation suites
        env:
          ENABLE_DENSE: "0"
          AE_BIND_PORT: "8001"
          AE_INDEX_DIR: ${{ github.workspace }}/data/index
        run: |
          make eval-defs
          make eval-concepts
          make eval-trouble

      - name: Upload evaluation artifacts
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-reports
          path: |
            eval_defs.json
            eval_concepts.json
            eval_trouble.json

  perf:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: eval
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ${{ env.PIP_CACHE }}
          key: pip-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}

      - name: Install (editable)
        run: pip install -e .

      - name: Cache RFC index
        uses: actions/cache@v4
        with:
          path: ${{ env.INDEX_DIR }}
          key: rfcindex-${{ hashFiles('data/index/manifest.json') }}
          restore-keys: |
            rfcindex-

      - name: Build index (if needed)
        run: |
          if [ ! -f "${INDEX_DIR}/sections.jsonl" ]; then
            ./scripts/sync_rfc_min.sh
            python scripts/build_index.py
          fi

      - name: Build and run Docker container
        env:
          ENABLE_DENSE: "0"
          AE_BIND_PORT: "8001"
          AE_INDEX_DIR: ${{ github.workspace }}/data/index
          AE_CACHE_ENABLED: "1"
          AE_CACHE_TTL_S: "60"
          AE_CACHE_SIZE: "512"
        run: |
          docker build -t aev2:test .
          docker run -d --name aev2-test \
            -p 8001:8001 \
            -v ${{ github.workspace }}/data:/app/data:ro \
            -e AE_BIND_PORT=8001 \
            -e ENABLE_DENSE=0 \
            -e AE_INDEX_DIR=/app/data/index \
            -e AE_CACHE_ENABLED=1 \
            -e AE_CACHE_TTL_S=60 \
            -e AE_CACHE_SIZE=512 \
            aev2:test

      - name: Wait for API readiness
        run: |
          timeout 60 bash -c 'until curl -sf http://localhost:8001/healthz; do sleep 2; done'

      - name: Run performance test
        run: |
          python scripts/perf_http.py --base http://localhost:8001 --total 50 --concurrency 8 --json perf_http.json

      - name: Check performance thresholds
        run: |
          p95_latency=$(jq -r '.latency_ms.p95' perf_http.json)
          echo "P95 latency: ${p95_latency}ms"
          if (( $(echo "$p95_latency > 250" | bc -l) )); then
            echo "ERROR: P95 latency ${p95_latency}ms exceeds 250ms threshold"
            exit 1
          fi
          echo "Performance test passed: P95 latency ${p95_latency}ms <= 250ms"

      - name: Check metrics presence
        run: |
          if [ -f "perf_metrics.prom" ]; then
            echo "Checking core metrics presence..."
            grep -q "ae_http_request_latency_ms" perf_metrics.prom || (echo "ERROR: ae_http_request_latency_ms not found" && exit 1)
            grep -q "ae_router_intent_total" perf_metrics.prom || (echo "ERROR: ae_router_intent_total not found" && exit 1)
            grep -q "ae_query_latency_ms" perf_metrics.prom || (echo "ERROR: ae_query_latency_ms not found" && exit 1)
            grep -q "ae_cache_hits_total" perf_metrics.prom || (echo "ERROR: ae_cache_hits_total not found" && exit 1)
            echo "All core metrics present"
          else
            echo "WARNING: perf_metrics.prom not found, skipping metrics check"
          fi

      - name: Upload performance artifacts
        uses: actions/upload-artifact@v4
        with:
          name: performance-reports
          path: |
            perf_http.json
            perf_metrics.prom
            perf_summary.json
