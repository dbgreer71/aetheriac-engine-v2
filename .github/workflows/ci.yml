name: CI

on: [push, pull_request]

env:
  PIP_CACHE: ~/.cache/pip
  INDEX_DIR: data/index
  ENABLE_DENSE: "0"
  AE_BIND_PORT: "8001"
  CONCEPT_MIN_SCORE: "0.05"

jobs:
  test-api:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      PYTHONUNBUFFERED: "1"
      PYTHONHASHSEED: "0"
      ENVIRONMENT: "development"
      DEBUG: "true"
      ENABLE_DENSE: "0"
      AE_BIND_PORT: "8001"
      AE_INDEX_DIR: "${{ github.workspace }}/data/index"
      AE_CACHE_ENABLED: "0"
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install (editable + test deps)
        run: |
          python -m pip install -U pip wheel
          python -m pip install -c constraints/ci.txt -e .
          python -m pip install -c constraints/ci.txt pytest

      - name: Prepare index
        run: |
          set -euo pipefail
          ./scripts/sync_rfc_min.sh
          python scripts/build_index.py
          ls -l data/index || true

      - name: CI diagnostics
        run: |
          python -V
          which python
          pip --version
          python -c "import sys,site,platform,os; print({'py':sys.version,'ex':sys.executable,'plat':platform.platform(), 'cwd':os.getcwd()})"
          pip freeze | tee ci_pip_freeze.txt
          python - <<'PY'
          import pkgutil, json, sys
          mods = sorted([m.name for m in pkgutil.iter_modules() if m.name.startswith('ae2')])
          print('AE2 modules:', mods)
          PY

      - name: Upload pip freeze
        uses: actions/upload-artifact@v4
        with:
          name: ci-pip-freeze
          path: ci_pip_freeze.txt

      - name: Run smoke tests
        run: |
          set -euo pipefail
          python -m pytest -q tests/ci_smoke_test.py -o log_cli=true -o log_cli_level=INFO | tee ci_pytest_smoke.log

      - name: Upload smoke test log
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ci-pytest-smoke
          path: ci_pytest_smoke.log

  ci-full:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: test-api
    continue-on-error: true
    env:
      PYTHONUNBUFFERED: "1"
      PYTHONHASHSEED: "0"
      ENVIRONMENT: "development"
      DEBUG: "true"
      ENABLE_DENSE: "0"
      AE_BIND_PORT: "8001"
      AE_INDEX_DIR: "${{ github.workspace }}/data/index"
      AE_CACHE_ENABLED: "0"
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install (editable)
        run: |
          python -m pip install -U pip wheel
          pip install -e . -c constraints/ci.txt

      - name: Prepare index
        run: |
          set -euo pipefail
          ./scripts/sync_rfc_min.sh
          python scripts/build_index.py

      - name: Run full test suite
        run: |
          set -euo pipefail
          pytest -q

  eval:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: test-api
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ${{ env.PIP_CACHE }}
          key: pip-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}

      - name: Install (editable)
        run: pip install -e .

      - name: Cache RFC index
        uses: actions/cache@v4
        with:
          path: ${{ env.INDEX_DIR }}
          key: rfcindex-${{ hashFiles('data/index/manifest.json') }}
          restore-keys: |
            rfcindex-

      - name: Build index (if needed)
        run: |
          if [ ! -f "${INDEX_DIR}/sections.jsonl" ]; then
            ./scripts/sync_rfc_min.sh
            python scripts/build_index.py
          fi

      - name: Run evaluation suites
        env:
          ENABLE_DENSE: "0"
          AE_BIND_PORT: "8001"
          AE_INDEX_DIR: ${{ github.workspace }}/data/index
        run: |
          make eval-defs
          make eval-concepts
          make eval-trouble

      - name: Upload evaluation artifacts
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-reports
          path: |
            eval_defs.json
            eval_concepts.json
            eval_trouble.json

  perf:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: eval
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ${{ env.PIP_CACHE }}
          key: pip-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}

      - name: Install (editable)
        run: pip install -e .

      - name: Cache RFC index
        uses: actions/cache@v4
        with:
          path: ${{ env.INDEX_DIR }}
          key: rfcindex-${{ hashFiles('data/index/manifest.json') }}
          restore-keys: |
            rfcindex-

      - name: Build index (if needed)
        run: |
          if [ ! -f "${INDEX_DIR}/sections.jsonl" ]; then
            ./scripts/sync_rfc_min.sh
            python scripts/build_index.py
          fi

      - name: Build and run Docker container
        env:
          ENABLE_DENSE: "0"
          AE_BIND_PORT: "8001"
          AE_INDEX_DIR: ${{ github.workspace }}/data/index
          AE_CACHE_ENABLED: "1"
          AE_CACHE_TTL_S: "60"
          AE_CACHE_SIZE: "512"
        run: |
          docker build -t aev2:test .
          docker run -d --name aev2-test \
            -p 8001:8001 \
            -v ${{ github.workspace }}/data:/app/data:ro \
            -e AE_BIND_PORT=8001 \
            -e ENABLE_DENSE=0 \
            -e AE_INDEX_DIR=/app/data/index \
            -e AE_CACHE_ENABLED=1 \
            -e AE_CACHE_TTL_S=60 \
            -e AE_CACHE_SIZE=512 \
            aev2:test

      - name: Wait for API readiness
        run: |
          timeout 60 bash -c 'until curl -sf http://localhost:8001/healthz; do sleep 2; done'

      - name: Run performance test
        run: |
          python scripts/perf_http.py --base http://localhost:8001 --total 50 --concurrency 8 --json perf_http.json

      - name: Check performance thresholds
        run: |
          p95_latency=$(jq -r '.latency_ms.p95' perf_http.json)
          echo "P95 latency: ${p95_latency}ms"
          if (( $(echo "$p95_latency > 250" | bc -l) )); then
            echo "ERROR: P95 latency ${p95_latency}ms exceeds 250ms threshold"
            exit 1
          fi
          echo "Performance test passed: P95 latency ${p95_latency}ms <= 250ms"

      - name: Check metrics presence
        run: |
          if [ -f "perf_metrics.prom" ]; then
            echo "Checking core metrics presence..."
            grep -q "ae_http_request_latency_ms" perf_metrics.prom || (echo "ERROR: ae_http_request_latency_ms not found" && exit 1)
            grep -q "ae_router_intent_total" perf_metrics.prom || (echo "ERROR: ae_router_intent_total not found" && exit 1)
            grep -q "ae_query_latency_ms" perf_metrics.prom || (echo "ERROR: ae_query_latency_ms not found" && exit 1)
            grep -q "ae_cache_hits_total" perf_metrics.prom || (echo "ERROR: ae_cache_hits_total not found" && exit 1)
            echo "All core metrics present"
          else
            echo "WARNING: perf_metrics.prom not found, skipping metrics check"
          fi

      - name: Upload performance artifacts
        uses: actions/upload-artifact@v4
        with:
          name: performance-reports
          path: |
            perf_http.json
            perf_metrics.prom
            perf_summary.json
